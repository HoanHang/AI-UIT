{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvindivAkg-9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 0. IMPORT & INSTALL LIBRARIES\n",
        "# ============================================\n",
        "# Install libraries\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "!pip install -q sentence-transformers scikit-learn xgboost\n",
        "!pip install pdfplumber python-docx spacy --quiet\n",
        "\n",
        "# --- general ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- import for part 3 ---\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib # for saving models\n",
        "\n",
        "# --- import for part 4 ---\n",
        "from google.colab import files\n",
        "import pdfplumber\n",
        "import docx\n",
        "\n",
        "# --- import for part 6: question generation ---\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ADDITIONAL FUNCTION: extract_technical_skills_from_skill2vec\n",
        "# Fix cho error \"function not defined\"\n",
        "# ============================================\n",
        "\n",
        "def extract_technical_skills_from_skill2vec():\n",
        "    \"\"\"\n",
        "    Alternative approach: Extract technical skills directly from skill2vec data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üîÑ Alternative extraction from skill2vec data...\")\n",
        "\n",
        "        # Load skill2vec data\n",
        "        url = \"https://raw.githubusercontent.com/duyet/skill2vec-dataset/master/skill2vec_10K.csv\"\n",
        "        df_skills = pd.read_csv(url, header=None, low_memory=False).iloc[:, 1:]\n",
        "\n",
        "        print(f\"üìä Processing skill2vec data: {df_skills.shape}\")\n",
        "\n",
        "        # Find technical skills from technical CVs\n",
        "        technical_skills = set()\n",
        "        technical_cvs_found = 0\n",
        "\n",
        "        for idx in range(min(200, len(df_skills))):\n",
        "            row_skills = df_skills.iloc[idx].dropna().tolist()\n",
        "            row_text = ' '.join(str(skill).lower() for skill in row_skills)\n",
        "\n",
        "            # Check if row has technical content\n",
        "            tech_indicators = [\n",
        "                'programming', 'development', 'software', 'web', 'database',\n",
        "                'python', 'java', 'javascript', 'sql', 'html', 'css',\n",
        "                'react', 'angular', 'node', 'api', 'framework', 'git', 'linux'\n",
        "            ]\n",
        "\n",
        "            tech_score = sum(1 for indicator in tech_indicators if indicator in row_text)\n",
        "\n",
        "            if tech_score >= 3:  # This is a technical CV\n",
        "                technical_cvs_found += 1\n",
        "\n",
        "                for skill in row_skills:\n",
        "                    skill_clean = str(skill).strip().lower()\n",
        "\n",
        "                    # Basic validation\n",
        "                    if (len(skill_clean) > 1 and len(skill_clean) < 30 and\n",
        "                        not any(noise in skill_clean for noise in [\n",
        "                            'training', 'management', 'experience', 'university',\n",
        "                            'company', 'years', 'level', 'senior', 'junior',\n",
        "                            'manager', 'analyst', 'director', 'coordinator'\n",
        "                        ])):\n",
        "                        technical_skills.add(skill_clean)\n",
        "\n",
        "        print(f\"üìä Found {technical_cvs_found} technical CVs\")\n",
        "        print(f\"üìä Extracted {len(technical_skills)} potential technical skills\")\n",
        "\n",
        "        # Filter to definitely technical skills only\n",
        "        definitely_technical = {\n",
        "            'python', 'java', 'javascript', 'typescript', 'react', 'angular', 'vue',\n",
        "            'html', 'css', 'sql', 'mysql', 'postgresql', 'mongodb', 'redis',\n",
        "            'aws', 'azure', 'docker', 'kubernetes', 'git', 'linux', 'node',\n",
        "            'tensorflow', 'pytorch', 'scikit-learn', 'pandas', 'numpy',\n",
        "            'bootstrap', 'jquery', 'webpack', 'elasticsearch', 'microservices',\n",
        "            'django', 'flask', 'spring', 'laravel', 'rails', 'express',\n",
        "            'cassandra', 'neo4j', 'jenkins', 'terraform', 'ansible',\n",
        "            'graphql', 'restful', 'oauth', 'jwt', 'ssl', 'api'\n",
        "        }\n",
        "\n",
        "        validated_skills = [skill for skill in technical_skills if skill in definitely_technical]\n",
        "\n",
        "        print(f\"‚úÖ Validated technical skills: {len(validated_skills)}\")\n",
        "        print(f\"üéØ Sample validated skills: {sorted(validated_skills)[:10]}\")\n",
        "\n",
        "        return validated_skills\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Alternative extraction error: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test the function\n",
        "print(\"üß™ Testing extract_technical_skills_from_skill2vec()...\")\n",
        "test_result = extract_technical_skills_from_skill2vec()\n",
        "print(f\"‚úÖ Function working! Found {len(test_result)} technical skills\")\n",
        "\n",
        "print(\"\\n‚úÖ Function extract_technical_skills_from_skill2vec() successfully added to notebook!\")\n"
      ],
      "metadata": {
        "id": "uU8wcVf9qYJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1. REPARE CV DATA FOR TRAINING REASON\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download NLTK resources\n",
        "for pkg in ['punkt', 'punkt_tab', 'stopwords']:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Import additional libraries for NLP\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "def read_dataset_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    return df\n",
        "\n",
        "# ============================================\n",
        "# ENHANCED SKILL EXTRACTOR CLASS\n",
        "# ============================================\n",
        "\n",
        "class EnhancedNLPSkillExtractor:\n",
        "    \"\"\"\n",
        "    H·ªá th·ªëng NLP ƒë√£ ƒë∆∞·ª£c train t·ª´ labeled job data\n",
        "    S·ª≠ d·ª•ng TF-IDF similarity cho skill extraction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.skill_database = set()\n",
        "        self.vectorizer = None\n",
        "        self.skill_vectors = None\n",
        "        self.skill_names = []\n",
        "        self.trained = False\n",
        "\n",
        "        print(\"üîÑ Initializing Enhanced NLP Skill Extractor...\")\n",
        "        self._load_or_train_models()\n",
        "\n",
        "    def _load_or_train_models(self):\n",
        "        \"\"\"Load trained models ho·∫∑c train n·∫øu ch∆∞a c√≥\"\"\"\n",
        "        try:\n",
        "            # Try load existing models (COLAB PATHS)\n",
        "            self.vectorizer = joblib.load('./content/fast_models/tfidf_vectorizer.pkl')\n",
        "            self.skill_vectors = joblib.load('./content/fast_models/skill_vectors.pkl')\n",
        "            self.skill_names = joblib.load('./content/fast_models/skill_names.pkl')\n",
        "            self.trained = True\n",
        "            print(f\"‚úÖ Loaded trained models: {len(self.skill_names)} skills\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è No trained models found, training from labeled data...\")\n",
        "            self._train_from_labeled_data()\n",
        "\n",
        "    def _train_from_labeled_data(self):\n",
        "        \"\"\"Train models t·ª´ labeled datasets\"\"\"\n",
        "        try:\n",
        "            # Load labeled datasets (COLAB PATHS)\n",
        "            df1 = pd.read_csv('./data/all_job_post.csv')\n",
        "            df2 = pd.read_csv('./data/JobsDatasetProcessed.csv')\n",
        "\n",
        "            print(f\"üìä Training from {df1.shape[0]} + {df2.shape[0]} labeled examples\")\n",
        "\n",
        "            # Build skill knowledge base\n",
        "            skill_contexts = defaultdict(list)\n",
        "\n",
        "            # Process dataset 1\n",
        "            for idx, row in df1.iterrows():\n",
        "                if pd.notna(row['job_description']) and pd.notna(row['job_skill_set']):\n",
        "                    text = str(row['job_description'])\n",
        "                    skills = self._parse_skills(row['job_skill_set'])\n",
        "\n",
        "                    for skill in skills:\n",
        "                        if self._is_technical_skill(skill):\n",
        "                            skill_contexts[skill].append(text)\n",
        "\n",
        "            # Process dataset 2\n",
        "            for idx, row in df2.iterrows():\n",
        "                if pd.notna(row['Description']) and pd.notna(row['IT Skills']):\n",
        "                    text = str(row['Description'])\n",
        "                    skills = self._parse_skills(row['IT Skills'])\n",
        "\n",
        "                    for skill in skills:\n",
        "                        if self._is_technical_skill(skill):\n",
        "                            skill_contexts[skill].append(text)\n",
        "\n",
        "            # Filter skills v·ªõi enough contexts\n",
        "            filtered_contexts = {skill: contexts for skill, contexts in skill_contexts.items()\n",
        "                               if len(contexts) >= 3}\n",
        "\n",
        "            print(f\"üéØ Training TF-IDF model on {len(filtered_contexts)} skills\")\n",
        "\n",
        "            # Train TF-IDF model\n",
        "            skill_docs = []\n",
        "            skill_names = []\n",
        "\n",
        "            for skill, contexts in filtered_contexts.items():\n",
        "                combined_context = ' '.join(contexts[:10])  # Max 10 contexts\n",
        "                skill_docs.append(combined_context)\n",
        "                skill_names.append(skill)\n",
        "\n",
        "            # TF-IDF vectorization\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=3000,\n",
        "                ngram_range=(1, 3),\n",
        "                stop_words='english',\n",
        "                lowercase=True,\n",
        "                min_df=1\n",
        "            )\n",
        "\n",
        "            skill_vectors = vectorizer.fit_transform(skill_docs)\n",
        "\n",
        "            # Save models (COLAB PATHS)\n",
        "            Path('/content/fast_models').mkdir(exist_ok=True)\n",
        "            joblib.dump(vectorizer, '/content/fast_models/tfidf_vectorizer.pkl')\n",
        "            joblib.dump(skill_vectors, '/content/fast_models/skill_vectors.pkl')\n",
        "            joblib.dump(skill_names, '/content/fast_models/skill_names.pkl')\n",
        "\n",
        "            self.vectorizer = vectorizer\n",
        "            self.skill_vectors = skill_vectors\n",
        "            self.skill_names = skill_names\n",
        "            self.trained = True\n",
        "\n",
        "            print(f\"‚úÖ Training completed: {len(skill_names)} skills\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed: {e}\")\n",
        "            self.trained = False\n",
        "\n",
        "    def _parse_skills(self, skill_string):\n",
        "        \"\"\"Parse skill string\"\"\"\n",
        "        if pd.isna(skill_string):\n",
        "            return []\n",
        "\n",
        "        skill_string = str(skill_string)\n",
        "        skill_string = re.sub(r'[\\'\"\\[\\]]', '', skill_string)\n",
        "        skills = re.split(r'[,;|]|\\sand\\s', skill_string)\n",
        "\n",
        "        cleaned = []\n",
        "        for skill in skills:\n",
        "            skill = skill.strip()\n",
        "            skill = re.sub(r'\\([^)]*\\)', '', skill)\n",
        "            skill = skill.strip()\n",
        "\n",
        "            if len(skill) > 1:\n",
        "                cleaned.append(skill.lower())\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def _is_technical_skill(self, skill):\n",
        "        \"\"\"Check if skill is technical\"\"\"\n",
        "        skill_lower = skill.lower()\n",
        "\n",
        "        # Exclude soft skills\n",
        "        soft_skills = {\n",
        "            'communication', 'teamwork', 'leadership', 'problem solving',\n",
        "            'time management', 'adaptability', 'collaboration'\n",
        "        }\n",
        "\n",
        "        if skill_lower in soft_skills:\n",
        "            return False\n",
        "\n",
        "        # Include technical indicators\n",
        "        tech_keywords = {\n",
        "            'programming', 'development', 'database', 'cloud', 'framework',\n",
        "            'python', 'java', 'javascript', 'sql', 'aws', 'docker', 'react'\n",
        "        }\n",
        "\n",
        "        return any(keyword in skill_lower for keyword in tech_keywords) or len(skill_lower.split()) <= 2\n",
        "\n",
        "    def extract_skills(self, text, top_k=15, threshold=0.15):\n",
        "        \"\"\"Extract skills using trained TF-IDF similarity\"\"\"\n",
        "        if not self.trained:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Vectorize input text\n",
        "            text_clean = str(text).lower()\n",
        "            text_vector = self.vectorizer.transform([text_clean])\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(text_vector, self.skill_vectors)[0]\n",
        "\n",
        "            # Get top similar skills\n",
        "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "            predicted_skills = []\n",
        "            for idx in top_indices:\n",
        "                similarity = similarities[idx]\n",
        "                if similarity >= threshold:\n",
        "                    skill = self.skill_names[idx]\n",
        "                    predicted_skills.append(skill)\n",
        "\n",
        "            return predicted_skills\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in skill extraction: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize enhanced extractor (ch·ªâ ch·∫°y 1 l·∫ßn)\n",
        "print(\"üöÄ Initializing Enhanced NLP Skill Extractor...\")\n",
        "enhanced_nlp_extractor = EnhancedNLPSkillExtractor()\n",
        "\n",
        "# ============================================\n",
        "# ORIGINAL SKILL LIST (Keep for compatibility)\n",
        "# ============================================\n",
        "SKILL_LIST = {\n",
        "    \"data structures\", \"algorithms\", \"python\", \"java\", \"c++\", \"c#\", \"rust\",\n",
        "    \"javascript\", \"typescript\", \"html\", \"css\", \"php\", \"ruby\", \"go\", \"swift\",\n",
        "    \"git\", \"version control\", \"testing\", \"unit testing\", \"database\",\n",
        "    \"mysql\", \"postgresql\", \"mongodb\", \"cloud computing\", \"aws\", \"azure\",\n",
        "    \"docker\", \"kubernetes\", \"ci/cd\", \"devops\", \"machine learning\",\n",
        "    \"deep learning\", \"nlp\", \"cybersecurity\", \"web development\",\n",
        "    \"mobile development\", \"android\", \"ios\", \"data analysis\", \"sql\",\n",
        "    \"problem solving\", \"linux\", \"shell scripting\"\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# SKILL2VEC INTEGRATION - Apply trained models to unlabeled data\n",
        "# S·ª≠ d·ª•ng trained NLP models ƒë·ªÉ m·ªü r·ªông SKILL_LIST t·ª´ skill2vec data\n",
        "# ============================================\n",
        "\n",
        "def clean_skill(s):\n",
        "    \"\"\"Enhanced clean skill function for skill2vec data\"\"\"\n",
        "    if not s or pd.isna(s):\n",
        "        return None\n",
        "\n",
        "    s = str(s).strip().lower()\n",
        "\n",
        "    # B·ªè r·ªóng\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    # Remove HTML entities v√† special characters\n",
        "    s = re.sub(r'&quot;|\"|&|<|>', '', s)\n",
        "    s = re.sub(r'[^\\w\\s\\.\\-\\+#]', ' ', s)  # Keep only alphanumeric, space, dot, dash, plus, hash\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()  # Multiple spaces to single\n",
        "\n",
        "    # B·ªè n·∫øu qu√° d√†i (likely spam/noise)\n",
        "    if len(s) > 50:\n",
        "        return None\n",
        "\n",
        "    # B·ªè n·∫øu ch·ª©a qu√° nhi·ªÅu keywords (spam detection)\n",
        "    spam_indicators = s.count(' or ') + s.count(' and ') + s.count('quot')\n",
        "    if spam_indicators > 3:\n",
        "        return None\n",
        "\n",
        "    # B·ªè job titles v√† non-technical terms\n",
        "    job_titles = {\n",
        "        'manager', 'analyst', 'director', 'coordinator', 'specialist',\n",
        "        'officer', 'assistant', 'supervisor', 'lead', 'senior',\n",
        "        'junior', 'intern', 'consultant', 'advisor', 'executive'\n",
        "    }\n",
        "\n",
        "    # Check if it's primarily a job title\n",
        "    words = s.split()\n",
        "    if len(words) <= 3 and any(title in s for title in job_titles):\n",
        "        # Keep only if it has technical context\n",
        "        tech_context = any(tech in s for tech in [\n",
        "            'software', 'system', 'data', 'network', 'security', 'cloud',\n",
        "            'web', 'mobile', 'database', 'api', 'development', 'engineering'\n",
        "        ])\n",
        "        if not tech_context:\n",
        "            return None\n",
        "\n",
        "    # Gi·ªØ ƒë·∫∑c bi·ªát C, R, Go, .net\n",
        "    special_keep = {'c', 'r', 'go', '.net', 'c++', 'c#', 'f#'}\n",
        "    if s in special_keep:\n",
        "        return s\n",
        "\n",
        "    # B·ªè s·ªë ho·∫∑c ch·ªâ k√Ω hi·ªáu\n",
        "    if re.fullmatch(r\"[\\d\\.\\-\\+]+\", s):\n",
        "        return None\n",
        "\n",
        "    # B·ªè t·ª´ 1 k√Ω t·ª± (tr·ª´ special cases)\n",
        "    if len(s) < 2:\n",
        "        return None\n",
        "\n",
        "    # B·ªè stopwords m·ªü r·ªông\n",
        "    stopwords = {\n",
        "        \"and\", \"or\", \"to\", \"of\", \"in\", \"on\", \"at\", \"for\", \"with\", \"by\",\n",
        "        \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "        \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"could\",\n",
        "        \"should\", \"may\", \"might\", \"must\", \"can\", \"this\", \"that\", \"these\", \"those\"\n",
        "    }\n",
        "\n",
        "    if s in stopwords:\n",
        "        return None\n",
        "\n",
        "    # B·ªè n·∫øu ch·ª©a suspicious patterns\n",
        "    suspicious_patterns = [\n",
        "        r'^\\d+[a-z]*$',     # Numbers with letters: 3gpp, 2d, 3d\n",
        "        r'_[a-z]',          # Underscore patterns: advertsing_campaign\n",
        "        r'maintenance$',    # Maintenance jobs\n",
        "        r'rendering$',      # Non-programming rendering\n",
        "        r'mobiles?$',       # Mobile devices/brands\n",
        "        r'campaign',        # Marketing campaigns\n",
        "        r'verification$',   # Testing/QA (unless software testing)\n",
        "        r'^adobe',          # Adobe products (unless technical)\n",
        "        r'^alcatel|^nokia|^samsung|^apple', # Device brands\n",
        "    ]\n",
        "\n",
        "    # Special exceptions for technical terms\n",
        "    tech_exceptions = [\n",
        "        'software verification', 'code verification', 'system verification',\n",
        "        'adobe flash', 'adobe air', 'mobile development', 'mobile app'\n",
        "    ]\n",
        "\n",
        "    # Check suspicious patterns but allow technical exceptions\n",
        "    is_suspicious = any(re.search(pattern, s) for pattern in suspicious_patterns)\n",
        "    is_exception = any(exception in s for exception in tech_exceptions)\n",
        "\n",
        "    if is_suspicious and not is_exception:\n",
        "        return None\n",
        "\n",
        "    # Additional filters for common noise\n",
        "    noise_terms = {\n",
        "        'services', 'solutions', 'systems', 'lab', 'laboratory',\n",
        "        'center', 'centre', 'office', 'department', 'division',\n",
        "        'group', 'team', 'unit', 'branch', 'facility'\n",
        "    }\n",
        "\n",
        "    # If it's just a generic term, filter it out\n",
        "    if len(words) <= 2 and any(noise in s for noise in noise_terms):\n",
        "        # Keep only if it has technical context\n",
        "        tech_indicators = [\n",
        "            'software', 'web', 'mobile', 'data', 'cloud', 'network',\n",
        "            'security', 'database', 'api', 'development', 'programming'\n",
        "        ]\n",
        "        if not any(tech in s for tech in tech_indicators):\n",
        "            return None\n",
        "\n",
        "    return s\n",
        "\n",
        "def validate_skill_with_trained_model(skill_candidate, context=\"\"):\n",
        "    \"\"\"\n",
        "    Enhanced validation using trained NLP model + additional filters\n",
        "    \"\"\"\n",
        "    if not enhanced_nlp_extractor.trained:\n",
        "        return False\n",
        "\n",
        "    skill_lower = skill_candidate.lower().strip()\n",
        "\n",
        "    # Pre-validation filters\n",
        "    if not _is_technical_skill_candidate(skill_lower):\n",
        "        return False\n",
        "\n",
        "    # Create test context for skill\n",
        "    test_text = f\"Experience with {skill_candidate} development and {skill_candidate} programming\"\n",
        "\n",
        "    # Use trained model ƒë·ªÉ check if skill is valid\n",
        "    try:\n",
        "        extracted_skills = enhanced_nlp_extractor.extract_skills(test_text, threshold=0.1)\n",
        "\n",
        "        # Check if similar skills are found\n",
        "        for extracted in extracted_skills:\n",
        "            if (skill_candidate.lower() in extracted.lower() or\n",
        "                extracted.lower() in skill_candidate.lower()):\n",
        "                return True\n",
        "\n",
        "        # Check against known technical patterns\n",
        "        tech_patterns = [\n",
        "            r'\\w+script', r'\\w+\\.js', r'\\w+\\.py', r'\\w+\\+\\+',\n",
        "            r'machine learning', r'data science', r'web development',\n",
        "            r'software', r'programming', r'development', r'framework',\n",
        "            r'database', r'server', r'cloud', r'api', r'sdk'\n",
        "        ]\n",
        "\n",
        "        pattern_match = any(re.search(pattern, skill_lower) for pattern in tech_patterns)\n",
        "\n",
        "        # Additional validation for common technical skills\n",
        "        known_technical = {\n",
        "            'python', 'java', 'javascript', 'typescript', 'react', 'angular', 'vue',\n",
        "            'node', 'express', 'django', 'flask', 'spring', 'laravel',\n",
        "            'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',\n",
        "            'docker', 'kubernetes', 'aws', 'azure', 'gcp', 'terraform',\n",
        "            'git', 'jenkins', 'ci/cd', 'devops', 'linux', 'windows',\n",
        "            'html', 'css', 'sass', 'bootstrap', 'jquery', 'webpack',\n",
        "            'tensorflow', 'pytorch', 'scikit-learn', 'pandas', 'numpy'\n",
        "        }\n",
        "\n",
        "        direct_match = skill_lower in known_technical\n",
        "\n",
        "        return pattern_match or direct_match\n",
        "\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def _is_technical_skill_candidate(skill):\n",
        "    \"\"\"Additional pre-filtering for technical skills\"\"\"\n",
        "\n",
        "    # Exclude obvious non-technical terms\n",
        "    non_technical_terms = {\n",
        "        'advertising', 'marketing', 'sales', 'finance', 'accounting',\n",
        "        'human resources', 'hr', 'recruiting', 'legal', 'compliance',\n",
        "        'administration', 'office', 'clerical', 'customer service',\n",
        "        'retail', 'hospitality', 'food service', 'healthcare',\n",
        "        'nursing', 'medical', 'pharmacy', 'dental', 'veterinary',\n",
        "        'education', 'teaching', 'training', 'coaching', 'mentoring',\n",
        "        'real estate', 'insurance', 'banking', 'investment',\n",
        "        'logistics', 'transportation', 'shipping', 'warehouse',\n",
        "        'manufacturing', 'production', 'quality control', 'assembly'\n",
        "    }\n",
        "\n",
        "    # Check if skill contains non-technical terms\n",
        "    if any(term in skill for term in non_technical_terms):\n",
        "        return False\n",
        "\n",
        "    # Exclude pure job titles without technical context\n",
        "    job_title_only = [\n",
        "        'manager', 'director', 'coordinator', 'specialist', 'analyst',\n",
        "        'officer', 'assistant', 'supervisor', 'lead', 'consultant'\n",
        "    ]\n",
        "\n",
        "    words = skill.split()\n",
        "    if len(words) <= 2 and any(title in skill for title in job_title_only):\n",
        "        return False\n",
        "\n",
        "    # Exclude industry-specific non-tech terms\n",
        "    industry_terms = {\n",
        "        'aircraft', 'automotive', 'construction', 'agriculture',\n",
        "        'textiles', 'fashion', 'beauty', 'cosmetics', 'jewelry',\n",
        "        'sports', 'fitness', 'entertainment', 'media', 'publishing',\n",
        "        'photography', 'video production', 'graphic design'\n",
        "    }\n",
        "\n",
        "    if any(term in skill for term in industry_terms):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def apply_trained_models_to_skill2vec():\n",
        "    \"\"\"\n",
        "    ƒê√öNG M·ª§C ƒê√çCH: S·ª≠ d·ª•ng trained models ƒë·ªÉ extract skills t·ª´ skill2vec (unlabeled data)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üéØ APPLYING TRAINED MODELS TO SKILL2VEC (UNLABELED DATA)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Load skill2vec data (raw, unlabeled)\n",
        "        url = \"https://raw.githubusercontent.com/duyet/skill2vec-dataset/master/skill2vec_10K.csv\"\n",
        "        df_skills = pd.read_csv(url, header=None, low_memory=False).iloc[:, 1:]\n",
        "\n",
        "        print(f\"üìä Skill2vec raw data: {df_skills.shape}\")\n",
        "        print(\"üéØ Goal: Use trained NLP models to extract skills from this unlabeled data\")\n",
        "\n",
        "        if not enhanced_nlp_extractor.trained:\n",
        "            print(\"‚ùå No trained models available! Please train first from labeled data.\")\n",
        "            return []\n",
        "\n",
        "        # Process skill2vec CVs using trained models\n",
        "        extracted_skills_from_unlabeled = set()\n",
        "        processed_cvs = 0\n",
        "\n",
        "        print(\"ü§ñ Applying trained models to extract skills from unlabeled CVs...\")\n",
        "\n",
        "        # Process each CV in skill2vec dataset\n",
        "        for idx in range(min(200, len(df_skills))):  # Process first 200 CVs\n",
        "            # Get all skills/text for this CV\n",
        "            cv_skills = df_skills.iloc[idx].dropna().tolist()\n",
        "            cv_text = ' '.join(str(skill) for skill in cv_skills)\n",
        "\n",
        "            # Skip if CV is too short or looks non-technical\n",
        "            if len(cv_text) < 50:\n",
        "                continue\n",
        "\n",
        "            # Use trained NLP model to extract skills from this unlabeled CV\n",
        "            try:\n",
        "                # Apply trained model to this unlabeled text\n",
        "                predicted_skills = enhanced_nlp_extractor.extract_skills(\n",
        "                    cv_text,\n",
        "                    top_k=20,\n",
        "                    threshold=0.2  # Lower threshold for discovery\n",
        "                )\n",
        "\n",
        "                # Add discovered skills to our database\n",
        "                for skill in predicted_skills:\n",
        "                    if len(skill) > 1 and len(skill) < 30:  # Basic validation\n",
        "                        extracted_skills_from_unlabeled.add(skill)\n",
        "\n",
        "                processed_cvs += 1\n",
        "\n",
        "                # Progress indicator\n",
        "                if processed_cvs % 20 == 0:\n",
        "                    print(f\"   Processed {processed_cvs} CVs, discovered {len(extracted_skills_from_unlabeled)} unique skills\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ TRANSFER LEARNING RESULTS:\")\n",
        "        print(f\"   - Processed {processed_cvs} unlabeled CVs from skill2vec\")\n",
        "        print(f\"   - Discovered {len(extracted_skills_from_unlabeled)} new skills using trained models\")\n",
        "\n",
        "        # Clean discovered skills\n",
        "        final_discovered_skills = []\n",
        "        for skill in extracted_skills_from_unlabeled:\n",
        "            # Apply basic cleaning\n",
        "            cleaned = clean_skill(skill)\n",
        "            if cleaned and is_valuable_discovered_skill(cleaned):\n",
        "                final_discovered_skills.append(cleaned)\n",
        "\n",
        "        print(f\"üìä Final discovered skills after cleaning: {len(final_discovered_skills)}\")\n",
        "\n",
        "        # Show sample discovered skills\n",
        "        print(f\"üÜï Sample skills discovered from unlabeled data:\")\n",
        "        for skill in sorted(final_discovered_skills)[:20]:\n",
        "            print(f\"   ‚Ä¢ {skill}\")\n",
        "\n",
        "        return final_discovered_skills\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error applying models to skill2vec: {e}\")\n",
        "        return []\n",
        "\n",
        "def is_valuable_discovered_skill(skill):\n",
        "    \"\"\"\n",
        "    Check if discovered skill adds value to our database\n",
        "    \"\"\"\n",
        "    skill_lower = skill.lower()\n",
        "\n",
        "    # Skip if already in original SKILL_LIST\n",
        "    original_static_skills = {\n",
        "        \"data structures\", \"algorithms\", \"python\", \"java\", \"c++\", \"c#\", \"rust\",\n",
        "        \"javascript\", \"typescript\", \"html\", \"css\", \"php\", \"ruby\", \"go\", \"swift\",\n",
        "        \"git\", \"version control\", \"testing\", \"unit testing\", \"database\",\n",
        "        \"mysql\", \"postgresql\", \"mongodb\", \"cloud computing\", \"aws\", \"azure\",\n",
        "        \"docker\", \"kubernetes\", \"ci/cd\", \"devops\", \"machine learning\",\n",
        "        \"deep learning\", \"nlp\", \"cybersecurity\", \"web development\",\n",
        "        \"mobile development\", \"android\", \"ios\", \"data analysis\", \"sql\",\n",
        "        \"problem solving\", \"linux\", \"shell scripting\"\n",
        "    }\n",
        "\n",
        "    if skill_lower in original_static_skills:\n",
        "        return False  # Already have this\n",
        "\n",
        "    # Only keep if it's clearly technical and adds value\n",
        "    valuable_patterns = [\n",
        "        r'.*framework.*', r'.*library.*', r'.*sdk.*', r'.*api.*',\n",
        "        r'.*server.*', r'.*database.*', r'.*platform.*', r'.*service.*',\n",
        "        r'.*development.*', r'.*programming.*', r'.*scripting.*'\n",
        "    ]\n",
        "\n",
        "    # Known valuable technical skills not in original list\n",
        "    valuable_additions = {\n",
        "        'tensorflow', 'pytorch', 'scikit-learn', 'pandas', 'numpy', 'matplotlib',\n",
        "        'react', 'angular', 'vue', 'node', 'express', 'django', 'flask',\n",
        "        'spring', 'laravel', 'rails', 'bootstrap', 'jquery', 'webpack',\n",
        "        'redis', 'elasticsearch', 'cassandra', 'neo4j', 'jenkins', 'terraform',\n",
        "        'ansible', 'puppet', 'helm', 'grafana', 'prometheus', 'elk',\n",
        "        'microservices', 'restful', 'graphql', 'oauth', 'jwt', 'ssl'\n",
        "    }\n",
        "\n",
        "    return (skill_lower in valuable_additions or\n",
        "            any(re.search(pattern, skill_lower) for pattern in valuable_patterns))\n",
        "\n",
        "# Apply trained models to skill2vec (correct approach)\n",
        "try:\n",
        "    skill2vec_discovered = apply_trained_models_to_skill2vec()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in transfer learning: {e}\")\n",
        "    skill2vec_discovered = []  # Empty fallback\n",
        "\n",
        "def clean_skill_strict(s):\n",
        "    \"\"\"\n",
        "    Stricter cleaning for skill2vec data\n",
        "    \"\"\"\n",
        "    if not s or pd.isna(s):\n",
        "        return None\n",
        "\n",
        "    s = str(s).strip().lower()\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    # Remove HTML entities v√† special characters\n",
        "    s = re.sub(r'&quot;|\"|&|<|>', '', s)\n",
        "    s = re.sub(r'[^\\w\\s\\.\\-\\+#]', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "    # Basic length check\n",
        "    if len(s) < 2 or len(s) > 30:  # Stricter length limit\n",
        "        return None\n",
        "\n",
        "    # Immediate rejection patterns\n",
        "    reject_patterns = [\n",
        "        r'^\\d+[a-z]*$',        # 3gpp, 2d, 3d\n",
        "        r'.*_.*',              # Underscore patterns\n",
        "        r'.*training.*',       # Training-related\n",
        "        r'.*management.*',     # Management-related\n",
        "        r'.*experience.*',     # Experience-related\n",
        "        r'.*ambulance.*',      # Medical\n",
        "        r'.*anaesth.*',        # Medical\n",
        "        r'.*american.*',       # Company names\n",
        "        r'.*university.*',     # Education\n",
        "        r'.*clinic.*',         # Medical\n",
        "        r'.*nursing.*',        # Medical\n",
        "        r'.*airline.*',        # Aviation\n",
        "        r'.*voice.*',          # Call center\n",
        "        r'.*accent.*',         # Call center\n",
        "        r'.*bpo.*',            # Business process\n",
        "        r'.*call centre.*',    # Call center\n",
        "    ]\n",
        "\n",
        "    if any(re.search(pattern, s) for pattern in reject_patterns):\n",
        "        return None\n",
        "\n",
        "    # Keep special programming languages\n",
        "    special_keep = {'c', 'r', 'go', '.net', 'c++', 'c#', 'f#'}\n",
        "    if s in special_keep:\n",
        "        return s\n",
        "\n",
        "    return s\n",
        "\n",
        "def is_definitely_technical_skill(skill):\n",
        "    \"\"\"\n",
        "    Final validation - only keep definitely technical skills\n",
        "    \"\"\"\n",
        "    skill_lower = skill.lower()\n",
        "\n",
        "    # Definite technical skills (whitelist approach)\n",
        "    definite_technical = {\n",
        "        # Programming languages\n",
        "        'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'go', 'rust',\n",
        "        'swift', 'kotlin', 'scala', 'ruby', 'php', 'perl', 'r', 'c', 'dart',\n",
        "        'objective-c', 'assembly', 'cobol', 'fortran', 'haskell', 'lua', 'matlab',\n",
        "\n",
        "        # Web technologies\n",
        "        'html', 'css', 'react', 'angular', 'vue', 'node', 'express', 'django',\n",
        "        'flask', 'spring', 'laravel', 'rails', 'asp.net', 'jquery', 'bootstrap',\n",
        "\n",
        "        # Databases\n",
        "        'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',\n",
        "        'oracle', 'sqlite', 'cassandra', 'dynamodb', 'neo4j',\n",
        "\n",
        "        # Cloud & DevOps\n",
        "        'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'terraform',\n",
        "        'ansible', 'puppet', 'chef', 'vagrant', 'helm', 'git', 'github', 'gitlab',\n",
        "\n",
        "        # Operating Systems\n",
        "        'linux', 'ubuntu', 'centos', 'windows', 'macos', 'unix', 'debian',\n",
        "\n",
        "        # Tools & IDEs\n",
        "        'vscode', 'intellij', 'eclipse', 'visual studio', 'sublime', 'atom',\n",
        "        'vim', 'emacs', 'nano', 'notepad++',\n",
        "\n",
        "        # Testing\n",
        "        'selenium', 'junit', 'pytest', 'jest', 'mocha', 'cypress',\n",
        "\n",
        "        # ML & AI\n",
        "        'tensorflow', 'pytorch', 'scikit-learn', 'pandas', 'numpy',\n",
        "        'matplotlib', 'opencv', 'nltk', 'spacy', 'keras',\n",
        "\n",
        "        # Mobile\n",
        "        'android', 'ios', 'react native', 'flutter', 'xamarin', 'ionic',\n",
        "\n",
        "        # Build tools\n",
        "        'webpack', 'babel', 'gulp', 'grunt', 'maven', 'gradle', 'npm', 'yarn'\n",
        "    }\n",
        "\n",
        "    # Exact match\n",
        "    if skill_lower in definite_technical:\n",
        "        return True\n",
        "\n",
        "    # Technical patterns (be very selective)\n",
        "    technical_patterns = [\n",
        "        r'^.*script$',          # JavaScript, TypeScript, etc.\n",
        "        r'^.*\\.js$',            # React.js, Vue.js, etc.\n",
        "        r'^.*\\.py$',            # Django.py, etc.\n",
        "        r'^.*\\+\\+$',            # C++, etc.\n",
        "        r'^.*framework$',       # Angular framework, etc.\n",
        "        r'^.*api$',             # REST API, etc.\n",
        "        r'^.*sdk$',             # Android SDK, etc.\n",
        "        r'^.*server$',          # Apache server, etc.\n",
        "        r'^.*database$',        # MySQL database, etc.\n",
        "    ]\n",
        "\n",
        "    # Must be short and match technical pattern\n",
        "    if len(skill_lower.split()) <= 2:\n",
        "        return any(re.match(pattern, skill_lower) for pattern in technical_patterns)\n",
        "\n",
        "    return False\n",
        "\n",
        "# Apply smart extraction\n",
        "try:\n",
        "    skill2vec_validated = extract_technical_skills_from_skill2vec()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in smart extraction: {e}\")\n",
        "    skill2vec_validated = [\"python\", \"java\", \"sql\", \"git\", \"javascript\"]  # fallback\n",
        "\n",
        "# ============================================\n",
        "# TRANSFER LEARNING: Merge discovered skills v·ªõi original SKILL_LIST\n",
        "# ============================================\n",
        "\n",
        "print(\"üîó TRANSFER LEARNING - Merging discovered skills with original SKILL_LIST...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"üìä SKILL SOURCES:\")\n",
        "print(f\"   1. Original static skills: 43\")\n",
        "print(f\"   2. Trained NLP model skills: {len(enhanced_nlp_extractor.skill_names) if enhanced_nlp_extractor.trained else 0}\")\n",
        "print(f\"   3. Discovered from skill2vec (unlabeled): {len(skill2vec_discovered)}\")\n",
        "\n",
        "# Combine all skill sources\n",
        "original_static = {\n",
        "    \"data structures\", \"algorithms\", \"python\", \"java\", \"c++\", \"c#\", \"rust\",\n",
        "    \"javascript\", \"typescript\", \"html\", \"css\", \"php\", \"ruby\", \"go\", \"swift\",\n",
        "    \"git\", \"version control\", \"testing\", \"unit testing\", \"database\",\n",
        "    \"mysql\", \"postgresql\", \"mongodb\", \"cloud computing\", \"aws\", \"azure\",\n",
        "    \"docker\", \"kubernetes\", \"ci/cd\", \"devops\", \"machine learning\",\n",
        "    \"deep learning\", \"nlp\", \"cybersecurity\", \"web development\",\n",
        "    \"mobile development\", \"android\", \"ios\", \"data analysis\", \"sql\",\n",
        "    \"problem solving\", \"linux\", \"shell scripting\"\n",
        "}\n",
        "\n",
        "# Merge: Original + Discovered from unlabeled data\n",
        "SKILL_LIST = original_static.union(set(skill2vec_discovered))\n",
        "\n",
        "print(f\"\\n‚úÖ FINAL SKILL_LIST COMPOSITION:\")\n",
        "print(f\"   - Original static skills: {len(original_static)}\")\n",
        "print(f\"   - Skills discovered from unlabeled data: {len(skill2vec_discovered)}\")\n",
        "print(f\"   - Total unique skills: {len(SKILL_LIST)}\")\n",
        "print(f\"   - Net addition: {len(SKILL_LIST) - len(original_static)} new skills\")\n",
        "\n",
        "# Show new skills discovered through transfer learning\n",
        "new_discovered_skills = set(skill2vec_discovered) - original_static\n",
        "\n",
        "if new_discovered_skills:\n",
        "    print(f\"\\nüÜï NEW SKILLS DISCOVERED FROM UNLABELED DATA (Transfer Learning):\")\n",
        "    for skill in sorted(list(new_discovered_skills))[:20]:\n",
        "        print(f\"   ‚Ä¢ {skill}\")\n",
        "\n",
        "    if len(new_discovered_skills) > 20:\n",
        "        print(f\"   ... and {len(new_discovered_skills) - 20} more\")\n",
        "else:\n",
        "    print(f\"\\nüìä No new skills discovered (all predicted skills were already in original list)\")\n",
        "\n",
        "print(f\"\\nüéì NLP APPROACH SUMMARY:\")\n",
        "print(f\"   1. ‚úÖ Supervised Learning: Train t·ª´ {len(enhanced_nlp_extractor.skill_names) if enhanced_nlp_extractor.trained else 0} labeled examples\")\n",
        "print(f\"   2. ‚úÖ Transfer Learning: Apply trained models to unlabeled skill2vec data\")\n",
        "print(f\"   3. ‚úÖ Knowledge Discovery: Found {len(new_discovered_skills)} new technical skills\")\n",
        "print(f\"   4. ‚úÖ Database Expansion: {len(original_static)} ‚Üí {len(SKILL_LIST)} skills\")\n",
        "\n",
        "print(f\"\\nüöÄ Enhanced NLP system ready!\")\n",
        "print(f\"SKILL_LIST now contains both original + discovered skills from unlabeled data!\")\n",
        "\n",
        "for S in sorted(list(SKILL_LIST))[:1000]:\n",
        "    print(f\"   ‚Ä¢ {S}\")"
      ],
      "metadata": {
        "id": "kUQ8_x5nqhqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# INSPECT SKILL_LIST - Xem t·∫•t c·∫£ skills c√≥ trong database\n",
        "# ============================================\n",
        "\n",
        "print(\"üîç SKILL_LIST INSPECTION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Basic stats\n",
        "print(f\"üìä SKILL DATABASE STATISTICS:\")\n",
        "print(f\"   Total skills: {len(SKILL_LIST)}\")\n",
        "print(f\"   Data type: {type(SKILL_LIST)}\")\n",
        "\n",
        "# Convert to sorted list for easier viewing\n",
        "skill_list_sorted = sorted(list(SKILL_LIST))\n",
        "\n",
        "print(f\"\\nüìã ALL SKILLS IN DATABASE ({len(skill_list_sorted)} total):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Display skills in numbered list\n",
        "for i, skill in enumerate(skill_list_sorted, 1):\n",
        "    print(f\"{i:3d}. {skill}\")\n",
        "\n",
        "    # Add separator every 20 skills for readability\n",
        "    if i % 20 == 0:\n",
        "        print()\n",
        "\n",
        "# Categorize skills by type\n",
        "print(f\"\\nüè∑Ô∏è SKILL CATEGORIES:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Programming languages\n",
        "programming_langs = [s for s in SKILL_LIST if any(lang in s.lower() for lang in [\n",
        "    'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'go', 'rust',\n",
        "    'swift', 'kotlin', 'scala', 'ruby', 'php', 'perl', 'r '\n",
        "])]\n",
        "\n",
        "# Databases\n",
        "databases = [s for s in SKILL_LIST if any(db in s.lower() for db in [\n",
        "    'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',\n",
        "    'database', 'oracle', 'sqlite'\n",
        "])]\n",
        "\n",
        "# Cloud & DevOps\n",
        "cloud_devops = [s for s in SKILL_LIST if any(cloud in s.lower() for cloud in [\n",
        "    'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'terraform',\n",
        "    'devops', 'ci/cd', 'cloud'\n",
        "])]\n",
        "\n",
        "# Web Technologies\n",
        "web_tech = [s for s in SKILL_LIST if any(web in s.lower() for web in [\n",
        "    'html', 'css', 'react', 'angular', 'vue', 'node', 'express', 'django',\n",
        "    'flask', 'spring', 'web', 'api', 'rest'\n",
        "])]\n",
        "\n",
        "# Machine Learning & AI\n",
        "ml_ai = [s for s in SKILL_LIST if any(ml in s.lower() for ml in [\n",
        "    'machine learning', 'deep learning', 'tensorflow', 'pytorch', 'scikit',\n",
        "    'pandas', 'numpy', 'ai', 'artificial intelligence', 'neural', 'nlp'\n",
        "])]\n",
        "\n",
        "print(f\"üêç Programming Languages ({len(programming_langs)}):\")\n",
        "for lang in sorted(programming_langs)[:10]:\n",
        "    print(f\"   ‚Ä¢ {lang}\")\n",
        "\n",
        "print(f\"\\nüóÑÔ∏è Databases ({len(databases)}):\")\n",
        "for db in sorted(databases)[:10]:\n",
        "    print(f\"   ‚Ä¢ {db}\")\n",
        "\n",
        "print(f\"\\n‚òÅÔ∏è Cloud & DevOps ({len(cloud_devops)}):\")\n",
        "for cloud in sorted(cloud_devops)[:10]:\n",
        "    print(f\"   ‚Ä¢ {cloud}\")\n",
        "\n",
        "print(f\"\\nüåê Web Technologies ({len(web_tech)}):\")\n",
        "for web in sorted(web_tech)[:10]:\n",
        "    print(f\"   ‚Ä¢ {web}\")\n",
        "\n",
        "print(f\"\\nü§ñ ML & AI ({len(ml_ai)}):\")\n",
        "for ml in sorted(ml_ai)[:10]:\n",
        "    print(f\"   ‚Ä¢ {ml}\")\n",
        "\n",
        "# Search functionality\n",
        "print(f\"\\nüîç SEARCH FUNCTIONALITY:\")\n",
        "print(\"To search for specific skills, use:\")\n",
        "print(\"search_term = 'python'\")\n",
        "print(\"matching_skills = [s for s in SKILL_LIST if search_term.lower() in s.lower()]\")\n",
        "print(\"print(f'Skills containing \\\"{search_term}\\\": {matching_skills}')\")\n",
        "\n",
        "# Sample searches\n",
        "sample_searches = ['python', 'web', 'data', 'cloud', 'machine']\n",
        "print(f\"\\nüéØ SAMPLE SEARCHES:\")\n",
        "for term in sample_searches:\n",
        "    matches = [s for s in SKILL_LIST if term.lower() in s.lower()]\n",
        "    print(f\"   '{term}': {len(matches)} matches - {matches[:3]}...\")\n",
        "\n",
        "print(f\"\\n‚úÖ SKILL_LIST inspection completed!\")\n",
        "print(f\"üìã Total: {len(SKILL_LIST)} skills available for CV matching\")"
      ],
      "metadata": {
        "id": "wDcYsQIsqnAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoWcz2bikI8K"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. REPARE CV DATA FOR TRAINING REASON\n",
        "# ============================================\n",
        "# Download NLTK resources\n",
        "for pkg in ['punkt', 'punkt_tab', 'stopwords']:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def read_dataset_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    return df\n",
        "\n",
        "# Clean skill dataset\n",
        "def clean_skill(s):\n",
        "    s = s.strip().lower()\n",
        "    # B·ªè r·ªóng\n",
        "    if not s:\n",
        "        return None\n",
        "    # Gi·ªØ ƒë·∫∑c bi·ªát C, R, Go, .net\n",
        "    special_keep = {'c', 'r', 'go', '.net'}\n",
        "    if s in special_keep:\n",
        "        return s\n",
        "    # B·ªè s·ªë ho·∫∑c ch·ªâ k√Ω hi·ªáu\n",
        "    if re.fullmatch(r\"[\\d\\.\\-]+\", s):\n",
        "        return None\n",
        "    # B·ªè t·ª´ 1 k√Ω t·ª±\n",
        "    if len(s) < 2:\n",
        "        return None\n",
        "    # B·ªè stopwords c∆° b·∫£n\n",
        "    stopwords = {\"and\", \"or\", \"to\", \"of\", \"in\", \"on\", \"a\", \"an\", \"the\", \"for\"}\n",
        "    if s in stopwords:\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "# ============================================\n",
        "# II. SKILL LIST (From multiple sources)\n",
        "# ============================================\n",
        "SKILL_LIST = {\n",
        "    \"data structures\", \"algorithms\", \"python\", \"java\", \"c++\", \"c#\", \"rust\",\n",
        "    \"javascript\", \"typescript\", \"html\", \"css\", \"php\", \"ruby\", \"go\", \"swift\",\n",
        "    \"git\", \"version control\", \"testing\", \"unit testing\", \"database\",\n",
        "    \"mysql\", \"postgresql\", \"mongodb\", \"cloud computing\", \"aws\", \"azure\",\n",
        "    \"docker\", \"kubernetes\", \"ci/cd\", \"devops\", \"machine learning\",\n",
        "    \"deep learning\", \"nlp\", \"cybersecurity\", \"web development\",\n",
        "    \"mobile development\", \"android\", \"ios\", \"data analysis\", \"sql\",\n",
        "    \"problem solving\", \"linux\", \"shell scripting\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    # L·ªçc t·ª´ skill2vec_10K.csv\n",
        "    url = \"https://raw.githubusercontent.com/duyet/skill2vec-dataset/master/skill2vec_10K.csv\"\n",
        "    df_skills = pd.read_csv(url, header=None, low_memory=False).iloc[:, 1:]\n",
        "    skill2vec_clean = sorted(\n",
        "        {clean_skill(str(skill)) for skill in df_skills.stack() if pd.notnull(skill)}\n",
        "        - {None}\n",
        "    )\n",
        "    print(f\"T·ªïng skill s·∫°ch: {len(skill2vec_clean)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Kh√¥ng th·ªÉ load skill2vec_10K.csv: {e}\")\n",
        "    skill2vec_set = [\"python\", \"java\", \"sql\", \"git\", \"javascript\"]  # fallback\n",
        "\n",
        "# Merge v√†o SKILL_LIST\n",
        "#SKILL_LIST = set(SKILL_LIST).union(set(skill2vec_clean)) #-> IN-PROGRESS\n",
        "print(f\"‚úÖ T·ªïng k·ªπ nƒÉng sau khi merge: {len(SKILL_LIST)}\")\n",
        "\n",
        "# ============================================\n",
        "# III. AUTO STOP WORDS VIA TF-IDF\n",
        "# ============================================\n",
        "def get_auto_stopwords_from_dataset(documents, top_n=20):\n",
        "    \"\"\"\n",
        "    üîπ Ph√°t hi·ªán stop words t·ª± ƒë·ªông d·ª±a v√†o IDF\n",
        "    documents: list ch·ª©a n·ªôi dung vƒÉn b·∫£n (m·ªói CV 1 string)\n",
        "    top_n: s·ªë l∆∞·ª£ng t·ª´ √≠t gi√° tr·ªã ph√¢n bi·ªát nh·∫•t (IDF th·∫•p nh·∫•t)\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(lowercase=True)\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    idf_values = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
        "    stop_words_auto = sorted(idf_values, key=idf_values.get)[:top_n]\n",
        "    return set(stop_words_auto)\n",
        "\n",
        "# ============================================\n",
        "# IV. TEXT PROCESSING PIPELINE\n",
        "# ============================================\n",
        "def advanced_text_processing(text, auto_stopwords=None):\n",
        "    cv_text_str = str(text) if not pd.isna(text) else \"\"\n",
        "\n",
        "    # Step 1: Tokenization\n",
        "    text_lower = cv_text_str.lower()\n",
        "    text_clean = re.sub(r'[^\\w\\s]', ' ', text_lower)\n",
        "    tokens = nltk.word_tokenize(text_clean)\n",
        "\n",
        "    # Step 2: Stop words removal (NLTK + Auto)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if auto_stopwords:\n",
        "        stop_words = stop_words.union(auto_stopwords)\n",
        "    tokens_filtered = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Step 3: NER\n",
        "    doc = nlp(cv_text_str)\n",
        "    ner_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Step 4: Skill Extraction\n",
        "    found_skills = set()\n",
        "    for skill in SKILL_LIST:\n",
        "        if skill.lower() in text_lower:\n",
        "            found_skills.add(skill)\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens_filtered,\n",
        "        \"ner_entities\": ner_entities,\n",
        "        \"skills\": sorted(found_skills)\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# V. READ CV DATASET\n",
        "# ============================================\n",
        "# ƒê∆∞·ªùng d·∫´n t·ªõi file CSV c·ªßa b·∫°n\n",
        "file_path = \"./data/resume.csv\"\n",
        "\n",
        "# ƒê·ªçc file CSV\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# L·∫•y c·ªôt Resume_str\n",
        "cv_texts = df['Resume_str'].tolist()  # danh s√°ch t·∫•t c·∫£ CV texts\n",
        "\n",
        "# ---------------------------- Logs data for testing purpose (can remove) ----------------------------\n",
        "cv_text = df.loc[0, 'Resume_str']    # CV ƒë·∫ßu ti√™n trong dataset\n",
        "\n",
        "print(cv_text[:500])  # in 500 k√Ω t·ª± ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra\n",
        "auto_stopwords = get_auto_stopwords_from_dataset(df[df.columns[0]].astype(str).tolist(), top_n=20)\n",
        "print(\"üìå Auto Stopwords:\", auto_stopwords)\n",
        "print(\"üìÑ CV (Original):\\n\", cv_text[:500])\n",
        "result = advanced_text_processing(cv_text)\n",
        "print(\"\\nüîπ Tokens (sample):\", result[\"tokens\"][:20])\n",
        "print(\"\\nüîπ NER Entities:\", result[\"ner_entities\"])\n",
        "print(\"\\nüîπ Extracted Skills:\", result[\"skills\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YmFKvFAmHs6"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 2. LOAD JOB DESCRIPTION\n",
        "# ============================================\n",
        "def analyze_job_description(jd_text, auto_stopwords=None):\n",
        "    return advanced_text_processing(jd_text, auto_stopwords=auto_stopwords)\n",
        "\n",
        "def compare_cv_and_jd(cv_result, jd_result):\n",
        "    skills_cv = set(cv_result['skills'])\n",
        "    skills_jd = set(jd_result['skills'])\n",
        "\n",
        "    matched_skills = skills_cv.intersection(skills_jd)\n",
        "    missing_skills = skills_jd - skills_cv\n",
        "\n",
        "    return {\n",
        "        \"matched_skills\": sorted(matched_skills),\n",
        "        \"missing_skills\": sorted(missing_skills)\n",
        "    }\n",
        "\n",
        "# Hard-code JD sample\n",
        "JOB_DESCRIPTION_SAMPLE = \"\"\"\n",
        "We are seeking a Junior Mobile Developer to join our dynamic team.\n",
        "Responsibilities include:\n",
        "- Developing and maintaining mobile applications on Android and iOS platforms.\n",
        "- Working with backend services and databases (MySQL, MongoDB, Firebase).\n",
        "- Collaborating with UI/UX designers to deliver high-quality products.\n",
        "- Using Git for version control and participating in Agile/Scrum development processes.\n",
        "\n",
        "Requirements:\n",
        "- Strong knowledge of Java, Kotlin, or Swift.\n",
        "- Familiarity with REST APIs and JSON.\n",
        "- Experience with cloud services like AWS or Azure is a plus.\n",
        "- Good problem-solving and communication skills.\n",
        "\"\"\"\n",
        "\n",
        "jd_result = analyze_job_description(JOB_DESCRIPTION_SAMPLE)\n",
        "comparison = compare_cv_and_jd(result, jd_result)\n",
        "\n",
        "# ---------------------------- Logs data for testing purpose (can remove) ----------------------------\n",
        "print(\"\\nSkills in CV:\", result[\"skills\"])\n",
        "print(\"\\nSkills in JD:\", jd_result[\"skills\"])\n",
        "print(\"\\nMatched Skills:\", comparison[\"matched_skills\"])\n",
        "print(\"\\nMissing Skills for Candidate:\", comparison[\"missing_skills\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MTjw5jwmp37"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# PART 3 2: TRAINING & INFERENCE PIPELINE (Colab)\n",
        "# Ph·∫ßn 3: Hu·∫•n luy·ªán & suy lu·∫≠n\n",
        "# ============================================\n",
        "# [VI] M√¥ h√¨nh embedding, gi·ªØ nguy√™n\n",
        "# [EN] Embedding model, unchanged\n",
        "EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# ============================================\n",
        "# Helper functions\n",
        "# ============================================\n",
        "def compute_skill_overlap(cv_skills, jd_skills):\n",
        "    # [VI] Tr·∫£ v·ªÅ s·ªë skill tr√πng v√† t·ªâ l·ªá tr√πng\n",
        "    # [EN] Return matched_count, match_ratio\n",
        "    set_cv = set([s.lower() for s in cv_skills])\n",
        "    set_jd = set([s.lower() for s in jd_skills])\n",
        "    matched = set_cv.intersection(set_jd)\n",
        "    missing_list = sorted(list(set_jd - set_cv))\n",
        "    matched_count = len(matched)\n",
        "    match_ratio = matched_count / (len(set_jd) + 1e-9)\n",
        "    return matched_count, match_ratio, sorted(list(matched)), sorted(list(set_jd - set_cv))\n",
        "\n",
        "def tfidf_cosine_similarity(text_a, text_b, vectorizer=None):\n",
        "    # [VI] TF-IDF cosine similarity\n",
        "    # [EN] TF-IDF cosine similarity\n",
        "    if vectorizer is None:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        X = vectorizer.fit_transform([text_a, text_b])\n",
        "    else:\n",
        "        X = vectorizer.transform([text_a, text_b])\n",
        "    v0 = X[0].toarray()[0]\n",
        "    v1 = X[1].toarray()[0]\n",
        "    denom = (np.linalg.norm(v0) * np.linalg.norm(v1) + 1e-9)\n",
        "    return float(np.dot(v0, v1)/denom), vectorizer\n",
        "\n",
        "def embedding_similarity(text_a, text_b, model=EMBED_MODEL):\n",
        "    # [VI] Sentence-transformer cosine similarity\n",
        "    # [EN] Sentence-transformer cosine similarity\n",
        "    emb_a = model.encode(text_a, convert_to_tensor=True)\n",
        "    emb_b = model.encode(text_b, convert_to_tensor=True)\n",
        "    sim = util.cos_sim(emb_a, emb_b).item()\n",
        "    return float(sim)\n",
        "\n",
        "def build_features_single(cv_text, cv_skills, jd_text, jd_skills, vectorizer=None):\n",
        "    cv_text_str = str(cv_text) if not pd.isna(cv_text) else \"\"\n",
        "    jd_text_str = str(jd_text) if not pd.isna(jd_text) else \"\"\n",
        "    # [VI] X√¢y d·ª±ng vector ƒë·∫∑c tr∆∞ng cho m·ªôt c·∫∑p CV-JD\n",
        "    # [EN] Build feature vector for a single CV-JD pair\n",
        "    num_skills_cv = len(cv_skills)\n",
        "    num_skills_jd = len(jd_skills)\n",
        "    matched_count, match_ratio, matched_list, missing_list = compute_skill_overlap(cv_skills, jd_skills)\n",
        "    tfidf_sim, vectorizer = tfidf_cosine_similarity(\" \".join(cv_skills) + \" \" + cv_text_str,\n",
        "                                                    \" \".join(jd_skills) + \" \" + jd_text_str,\n",
        "                                                    vectorizer=vectorizer)\n",
        "    emb_sim = embedding_similarity(cv_text_str, jd_text_str)\n",
        "    cv_len_tokens = len(re.findall(r'\\w+', cv_text_str))\n",
        "    jd_len_tokens = len(re.findall(r'\\w+', jd_text_str))\n",
        "\n",
        "    features = {\n",
        "        \"num_skills_cv\": num_skills_cv,\n",
        "        \"num_skills_jd\": num_skills_jd,\n",
        "        \"matched_count\": matched_count,\n",
        "        \"match_ratio\": match_ratio,\n",
        "        \"tfidf_sim\": tfidf_sim,\n",
        "        \"emb_sim\": emb_sim,\n",
        "        \"cv_len_tokens\": cv_len_tokens,\n",
        "        \"jd_len_tokens\": jd_len_tokens,\n",
        "    }\n",
        "    meta = {\"matched_list\": matched_list, \"missing_list\": missing_list}\n",
        "    return features, vectorizer, meta\n",
        "\n",
        "# ============================================\n",
        "# [VI] Pipeline ch√≠nh: HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å\n",
        "# [EN] Main Pipeline: TRAINING & EVALUATION\n",
        "# ============================================\n",
        "def train_and_evaluate_models(X, y, models, test_size=0.4):\n",
        "    \"\"\"\n",
        "    [VI] Hu·∫•n luy·ªán v√† ƒë√°nh gi√° nhi·ªÅu m√¥ h√¨nh tr√™n c√πng m·ªôt t·∫≠p d·ªØ li·ªáu.\n",
        "    [EN] Train and evaluate multiple models on the same dataset.\n",
        "    \"\"\"\n",
        "    # [VI] Chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán\n",
        "    # [EN] Standardize the data before training\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # [VI] Gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng PCA (n·∫øu c·∫ßn)\n",
        "    # [EN] Reduce data dimensionality with PCA (if needed)\n",
        "    # PCA gi√∫p m√¥ h√¨nh Neural Network ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi √≠t ƒë·∫∑c tr∆∞ng h∆°n\n",
        "    # PCA helps the Neural Network model perform better with fewer features\n",
        "    if X_scaled.shape[1] > 5: # Ch·ªâ √°p d·ª•ng PCA n·∫øu c√≥ nhi·ªÅu h∆°n 5 ƒë·∫∑c tr∆∞ng\n",
        "        pca = PCA(n_components=0.95) # gi·ªØ l·∫°i 95% ph∆∞∆°ng sai\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "        print(f\"[VI] √Åp d·ª•ng PCA: {X_scaled.shape[1]} -> {X_pca.shape[1]} chi·ªÅu\")\n",
        "        print(f\"[EN] Applied PCA: {X_scaled.shape[1]} -> {X_pca.shape[1]} dimensions\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_pca, y, test_size=test_size, random_state=42\n",
        "        )\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n[VI] Hu·∫•n luy·ªán m√¥ h√¨nh: {name}...\")\n",
        "        print(f\"[EN] Training model: {name}...\")\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "            # [VI] ƒê√°nh gi√°\n",
        "            # [EN] Evaluation\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "            auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
        "\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            results[name] = {\n",
        "                \"model\": model,\n",
        "                \"accuracy\": accuracy,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall,\n",
        "                \"fscore\": fscore,\n",
        "                \"auc\": auc,\n",
        "                \"y_pred\": y_pred,\n",
        "                \"y_test\": y_test,\n",
        "                \"X_test_transformed\": X_test,\n",
        "                \"scaler\": scaler,\n",
        "                \"pca\": pca if 'pca' in locals() else None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"[VI] L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh {name}: {e}\")\n",
        "            print(f\"[EN] Error training model {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# [VI] C√ÅC H√ÄM TR·ª∞C QUAN H√ìA\n",
        "# ============================================\n",
        "def visualize_results(results):\n",
        "    \"\"\"\n",
        "    [VI] Tr·ª±c quan h√≥a c√°c k·∫øt qu·∫£ ƒë√°nh gi√° c·ªßa c√°c m√¥ h√¨nh.\n",
        "    [EN] Visualize the evaluation results of the models.\n",
        "    \"\"\"\n",
        "    metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\"]\n",
        "    model_names = list(results.keys())\n",
        "    scores = {\n",
        "        \"Accuracy\": [results[name][\"accuracy\"] for name in model_names],\n",
        "        \"Precision\": [results[name][\"precision\"] for name in model_names],\n",
        "        \"Recall\": [results[name][\"recall\"] for name in model_names],\n",
        "        \"F1-Score\": [results[name][\"fscore\"] for name in model_names],\n",
        "        \"AUC\": [results[name][\"auc\"] for name in model_names],\n",
        "    }\n",
        "\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.15\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    for i, (metric, values) in enumerate(scores.items()):\n",
        "        rects = ax.bar(x + (i - 2) * width, values, width, label=metric)\n",
        "        autolabel(rects, ax)\n",
        "\n",
        "    ax.set_ylabel(\"[VI] ƒêi·ªÉm s·ªë | [EN] Score\")\n",
        "    ax.set_title(\"[VI] So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh | [EN] Model Performance Comparison\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.legend(loc='lower right')\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def autolabel(rects, ax):\n",
        "    \"\"\"[VI] Ghi s·ªë l√™n ƒë·∫ßu c√°c c·ªôt | [EN] Attach a text label above each bar.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f\"{height:.2f}\",\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom',\n",
        "                    fontsize=8)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
        "    \"\"\"\n",
        "    [VI] V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n\n",
        "    [EN] Plot confusion matrix\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"[VI] D·ª± ƒëo√°n | [EN] Predicted\")\n",
        "    ax.set_ylabel(\"[VI] Th·ª±c t·∫ø | [EN] Actual\")\n",
        "\n",
        "# ============================================\n",
        "# EXECUTION\n",
        "# [VI] TH·ª∞C THI\n",
        "# ============================================\n",
        "# [VI] ...\n",
        "# [EN] Build df_pairs by the cv_texts have extracted from the previous part\n",
        "# (Code ƒë·ªÉ t·∫°o df_cv ƒë√£ c√≥ trong ph·∫ßn tr∆∞·ªõc)\n",
        "# --- Step 1: Load CV dataset ---\n",
        "df_cv = pd.DataFrame({'cv_text': cv_texts})\n",
        "df_cv = df_cv.sample(min(1000, len(df_cv)), random_state=42).reset_index(drop=True)\n",
        "print(df_cv.head(2))\n",
        "\n",
        "# --- Step 2: T·∫°o c√°c c·∫∑p CV-JD ---\n",
        "jd_text = JOB_DESCRIPTION_SAMPLE\n",
        "jd_skills = jd_result[\"skills\"]\n",
        "\n",
        "pairs = []\n",
        "for idx, row in df_cv.iterrows():\n",
        "    cv_text = row['cv_text']\n",
        "    cv_skills = advanced_text_processing(cv_text)[\"skills\"]\n",
        "    label = random.choice([0, 1])\n",
        "    pairs.append({\n",
        "        \"cv_text\": cv_text,\n",
        "        \"cv_skills\": cv_skills,\n",
        "        \"jd_text\": jd_text,\n",
        "        \"jd_skills\": jd_skills,\n",
        "        \"label\": label\n",
        "    })\n",
        "\n",
        "df_pairs = pd.DataFrame(pairs)  # <-- quan tr·ªçng, convert sang DataFrame\n",
        "\n",
        "# [VI] X√¢y d·ª±ng features cho to√†n b·ªô d·ªØ li·ªáu\n",
        "# [EN] Build features for the entire dataset\n",
        "feature_rows = []\n",
        "vectorizer = None\n",
        "metas = []\n",
        "for idx, row in df_pairs.iterrows():\n",
        "    feats, vectorizer, meta = build_features_single(\n",
        "        row.cv_text,\n",
        "        row.cv_skills,\n",
        "        row.jd_text,\n",
        "        row.jd_skills,\n",
        "        vectorizer=vectorizer\n",
        "    )\n",
        "    feature_rows.append(feats)\n",
        "    metas.append(meta)\n",
        "\n",
        "X = pd.DataFrame(feature_rows)\n",
        "y = df_pairs['label'].values\n",
        "\n",
        "print(\"[VI] B·∫£ng features sau khi tr√≠ch xu·∫•t:\")\n",
        "print(\"[EN] Features table after extraction:\")\n",
        "display(X.head())\n",
        "\n",
        "# [VI] ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh b·∫°n ƒë√£ h·ªçc\n",
        "# [EN] Define the models you have studied\n",
        "models_to_test = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Neural Network (MLP)\": MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42),\n",
        "}\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "def plot_roc_curves(results):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for name, res in results.items():\n",
        "        model = res['model']\n",
        "        X_test = res['X_test_transformed']  # ƒë√£ scale/PCA\n",
        "        try:\n",
        "            y_proba = model.predict_proba(X_test)[:,1]\n",
        "            fpr, tpr, _ = roc_curve(res['y_test'], y_proba)\n",
        "            plt.plot(fpr, tpr, label=f\"{name} (AUC={res['auc']:.2f})\")\n",
        "        except:\n",
        "            continue\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# [VI] Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
        "# [EN] Train and evaluate\n",
        "if 'label' in df_pairs.columns and len(np.unique(y)) > 1:\n",
        "    evaluation_results = train_and_evaluate_models(X, y, models_to_test)\n",
        "\n",
        "    # [VI] Tr·ª±c quan h√≥a k·∫øt qu·∫£ t·ªïng qu√°t\n",
        "    # [EN] Visualize overall results\n",
        "    visualize_results(evaluation_results)\n",
        "\n",
        "    # [VI] Tr·ª±c quan h√≥a chi ti·∫øt t·ª´ng m√¥ h√¨nh (ma tr·∫≠n nh·∫ßm l·∫´n)\n",
        "    # [EN] Visualize detailed results for each model (confusion matrix)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    for i, (name, res) in enumerate(evaluation_results.items()):\n",
        "        plot_confusion_matrix(res['y_test'], res['y_pred'], f\"Confusion Matrix: {name}\", axes[i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Tr·ª±c quan h√≥a ROC curve\n",
        "    plot_roc_curves(evaluation_results)\n",
        "\n",
        "    # [VI] L∆∞u m√¥ h√¨nh v√† scaler t·ªët nh·∫•t\n",
        "    # [EN] Save the best model and scaler\n",
        "    best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['fscore'])\n",
        "    best_model = evaluation_results[best_model_name]['model']\n",
        "    best_scaler = evaluation_results[best_model_name]['scaler']\n",
        "    best_pca = evaluation_results[best_model_name]['pca']\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c trained_model n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "    os.makedirs('trained_model', exist_ok=True)\n",
        "    model_path = os.path.join('trained_model', 'best_model.pkl')\n",
        "    scaler_path = os.path.join('trained_model', 'scaler.pkl')\n",
        "    pca_path = os.path.join('trained_model', 'pca.pkl')\n",
        "\n",
        "    # L∆∞u\n",
        "    joblib.dump(best_model, model_path)\n",
        "    joblib.dump(best_scaler, scaler_path)\n",
        "    if best_pca:\n",
        "        joblib.dump(best_pca, pca_path)\n",
        "\n",
        "    print(f\"\\n[VI] M√¥ h√¨nh t·ªët nh·∫•t (d·ª±a tr√™n F1-Score) l√†: {best_model_name}\")\n",
        "    print(f\"[EN] The best model (based on F1-Score) is: {best_model_name}\")\n",
        "else:\n",
        "    print(\"[VI] Kh√¥ng ƒë·ªß d·ªØ li·ªáu nh√£n ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.\")\n",
        "    print(\"[EN] Not enough labeled data to train the models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**DEMO WITH BEST SAVED MODEL**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nsOC7Zhd5Wq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PART 4.2: INFERENCE PIPELINE (Colab)\n",
        "# Ch·∫°y d·ª± ƒëo√°n CV b·∫±ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán s·∫µn\n",
        "# ============================================================\n",
        "\n",
        "import os, re, joblib, shutil, docx, pdfplumber\n",
        "import numpy as np, pandas as pd\n",
        "import nltk, spacy\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# ----------- LOAD MODELS -----------\n",
        "# SpaCy: NER + Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Embedding models\n",
        "EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "MULTI_EMBED_MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Download NLTK resources (stopwords, tokenizer)\n",
        "for pkg in ['punkt', 'stopwords']:\n",
        "    nltk.download(pkg, quiet=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEXT PROCESSING HELPERS | H√ÄM X·ª¨ L√ù VƒÇN B·∫¢N\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic cleaning: lowercase + remove special chars\n",
        "       Ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n: ch·ªØ th∆∞·ªùng + b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát\"\"\"\n",
        "    if pd.isna(text): return \"\"\n",
        "    return re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
        "\n",
        "def advanced_text_processing(text, auto_stopwords=None):\n",
        "    \"\"\"Full text pipeline: tokenize, stopword removal, NER, skill extraction\n",
        "       Pipeline x·ª≠ l√Ω vƒÉn b·∫£n: tokenize, lo·∫°i stopwords, NER, tr√≠ch xu·∫•t k·ªπ nƒÉng\"\"\"\n",
        "    text_clean = clean_text(text)\n",
        "    tokens = nltk.word_tokenize(text_clean)\n",
        "\n",
        "    # Stopword filtering\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if auto_stopwords:\n",
        "        stop_words |= set(auto_stopwords)\n",
        "    tokens_filtered = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # NER\n",
        "    doc = nlp(str(text))\n",
        "    ner_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Skill extraction\n",
        "    found_skills = {skill for skill in SKILL_LIST if skill.lower() in text_clean}\n",
        "\n",
        "    return {\"tokens\": tokens_filtered, \"ner_entities\": ner_entities, \"skills\": sorted(found_skills)}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FEATURE ENGINEERING | X√ÇY D·ª∞NG ƒê·∫∂C TR∆ØNG\n",
        "# ============================================================\n",
        "\n",
        "def compute_skill_overlap(cv_skills, jd_skills):\n",
        "    \"\"\"Skill overlap between CV and JD | So s√°nh k·ªπ nƒÉng gi·ªØa CV v√† JD\"\"\"\n",
        "    set_cv, set_jd = {s.lower() for s in cv_skills}, {s.lower() for s in jd_skills}\n",
        "    matched = set_cv & set_jd\n",
        "    return len(matched), len(matched) / (len(set_jd) + 1e-9), sorted(matched), sorted(set_jd - set_cv)\n",
        "\n",
        "def tfidf_cosine_similarity(text_a, text_b, vectorizer=None):\n",
        "    \"\"\"TF-IDF cosine similarity | ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng TF-IDF\"\"\"\n",
        "    if not text_a.strip() or not text_b.strip():\n",
        "        return 0.0, vectorizer  # n·∫øu text r·ªóng th√¨ similarity = 0\n",
        "\n",
        "    if vectorizer is None:\n",
        "        vectorizer = TfidfVectorizer(lowercase=True)\n",
        "        X = vectorizer.fit_transform([text_a, text_b])\n",
        "    else:\n",
        "        X = vectorizer.transform([text_a, text_b])\n",
        "    v0, v1 = X[0].toarray()[0], X[1].toarray()[0]\n",
        "    return float(np.dot(v0, v1) / (np.linalg.norm(v0) * np.linalg.norm(v1) + 1e-9)), vectorizer\n",
        "\n",
        "def embedding_similarity(text_a, text_b, model=EMBED_MODEL):\n",
        "    \"\"\"Embedding similarity using sentence-transformers | T∆∞∆°ng ƒë·ªìng embedding\"\"\"\n",
        "    emb_a, emb_b = model.encode(text_a, convert_to_tensor=True), model.encode(text_b, convert_to_tensor=True)\n",
        "    return float(util.cos_sim(emb_a, emb_b).item())\n",
        "\n",
        "def build_features_single(cv_text, cv_skills, jd_text, jd_skills, vectorizer=None):\n",
        "    \"\"\"Create feature vector for CV-JD pair | T·∫°o vector ƒë·∫∑c tr∆∞ng cho c·∫∑p CV-JD\"\"\"\n",
        "    cv_str, jd_str = str(cv_text or \"\"), str(jd_text or \"\")\n",
        "\n",
        "    # Skills overlap\n",
        "    matched_count, match_ratio, matched_list, missing_list = compute_skill_overlap(cv_skills, jd_skills)\n",
        "\n",
        "    # Text similarities\n",
        "    tfidf_sim, vectorizer = tfidf_cosine_similarity(\" \".join(cv_skills) + \" \" + cv_str,\n",
        "                                                    \" \".join(jd_skills) + \" \" + jd_str,\n",
        "                                                    vectorizer=vectorizer)\n",
        "    emb_sim = embedding_similarity(cv_str, jd_str)\n",
        "\n",
        "    features = {\n",
        "        \"num_skills_cv\": len(cv_skills),\n",
        "        \"num_skills_jd\": len(jd_skills),\n",
        "        \"matched_count\": matched_count,\n",
        "        \"match_ratio\": match_ratio,\n",
        "        \"tfidf_sim\": tfidf_sim,\n",
        "        \"emb_sim\": emb_sim,\n",
        "        \"cv_len_tokens\": len(re.findall(r'\\w+', cv_str)),\n",
        "        \"jd_len_tokens\": len(re.findall(r'\\w+', jd_str))\n",
        "    }\n",
        "    return features, vectorizer, {\"matched_list\": matched_list, \"missing_list\": missing_list}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL LOADING | T·∫¢I M√î H√åNH\n",
        "# ============================================================\n",
        "\n",
        "def load_trained_model(trained_model_dir=\"trained_model\"):\n",
        "    \"\"\"Load trained model, scaler, PCA | T·∫£i m√¥ h√¨nh, scaler, PCA ƒë√£ train\"\"\"\n",
        "    model = joblib.load(os.path.join(trained_model_dir, \"best_model.pkl\"))\n",
        "    scaler = joblib.load(os.path.join(trained_model_dir, \"scaler.pkl\"))\n",
        "    pca_path = os.path.join(trained_model_dir, \"pca.pkl\")\n",
        "    pca = joblib.load(pca_path) if os.path.exists(pca_path) else None\n",
        "    return model, scaler, pca\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INFERENCE | D·ª∞ ƒêO√ÅN\n",
        "# ============================================================\n",
        "\n",
        "def predict_cv_dataframe(df_cv, jd_text, jd_skills, trained_model_dir=\"trained_model\"):\n",
        "    \"\"\"Predict CV-JD fit from dataframe | D·ª± ƒëo√°n CV c√≥ ph√π h·ª£p JD kh√¥ng\"\"\"\n",
        "    auto_stopwords = get_auto_stopwords_from_dataset(df_cv.iloc[:, 0].astype(str).tolist(), top_n=20)\n",
        "    model, scaler, pca = load_trained_model(trained_model_dir)\n",
        "    vectorizer, results = None, []\n",
        "\n",
        "    for _, row in df_cv.iterrows():\n",
        "        cv_text = row['Resume_str']\n",
        "        cv_skills = advanced_text_processing(cv_text, auto_stopwords)[\"skills\"]\n",
        "\n",
        "        feats, vectorizer, _ = build_features_single(cv_text, cv_skills, jd_text, jd_skills, vectorizer)\n",
        "        X_scaled = scaler.transform(pd.DataFrame([feats]))\n",
        "        if pca: X_scaled = pca.transform(X_scaled)\n",
        "\n",
        "        pred_label = model.predict(X_scaled)[0]\n",
        "        pred_proba = model.predict_proba(X_scaled)[0, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        results.append({\"cv_text\": cv_text, \"pred_label\": pred_label, \"pred_proba\": pred_proba, \"extracted_skills\": cv_skills})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FILE HANDLER | X·ª¨ L√ù FILE CV\n",
        "# ============================================================\n",
        "\n",
        "def read_cv_file(file_path):\n",
        "    \"\"\"Read CV file (.pdf, .docx, .txt) ‚Üí text | ƒê·ªçc CV t·ª´ file sang text\"\"\"\n",
        "    ext = os.path.splitext(file_path)[-1].lower()\n",
        "    text = \"\"\n",
        "    if ext == \".pdf\":\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\\n\".join([p.extract_text() or \"\" for p in pdf.pages])\n",
        "    elif ext == \".docx\":\n",
        "        text = \"\\n\".join([para.text for para in docx.Document(file_path).paragraphs])\n",
        "    elif ext == \".txt\":\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f: text = f.read()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format | ƒê·ªãnh d·∫°ng file kh√¥ng h·ªó tr·ª£\")\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QUESTION SUGGESTION | G·ª¢I √ù C√ÇU H·ªéI\n",
        "# ============================================================\n",
        "\n",
        "def print_matched_questions(cv_embedding, question_embeddings, questions_df, threshold=0.25, top_k=10):\n",
        "    \"\"\"Suggest most relevant questions from CV | G·ª£i √Ω c√¢u h·ªèi ph√π h·ª£p t·ª´ CV\"\"\"\n",
        "    results, seen = util.semantic_search(cv_embedding, question_embeddings, top_k=top_k)[0], set()\n",
        "\n",
        "    for r in results:\n",
        "        if r['score'] < threshold: continue\n",
        "        q_row = questions_df.iloc[r['corpus_id']]\n",
        "        question_col = \"question\" if \"question\" in q_row else \"Question\"\n",
        "        q_text = q_row[question_col]\n",
        "\n",
        "        if q_text in seen: continue\n",
        "        seen.add(q_text)\n",
        "\n",
        "        print(f\"Score: {r['score']:.3f}\\nQuestion: {q_text}\")\n",
        "        if \"category\" in q_row or \"Category\" in q_row:\n",
        "            print(f\"Category: {q_row.get('category', q_row.get('Category'))}\")\n",
        "        if \"role\" in q_row: print(f\"Role: {q_row['role']}\")\n",
        "        if \"experience\" in q_row: print(f\"Experience: {q_row['experience']}\")\n",
        "        if \"difficulty\" in q_row or \"Difficulty\" in q_row:\n",
        "            print(f\"Difficulty: {q_row.get('difficulty', q_row.get('Difficulty'))}\")\n",
        "        if \"Answer\" in q_row: print(f\"Answer: {q_row['Answer']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# EXECUTION (UPLOAD + PREDICTION) | TH·ª∞C THI (UPLOAD + D·ª∞ ƒêO√ÅN)\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import files  # Only for Colab | Ch·ªâ d√πng tr√™n Colab\n",
        "\n",
        "# Upload CV\n",
        "os.makedirs(\"uploaded_cv\", exist_ok=True)\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "file_path = os.path.join(\"uploaded_cv\", file_name)\n",
        "shutil.move(file_name, file_path)\n",
        "\n",
        "# Read file ‚Üí DataFrame\n",
        "cv_text = read_cv_file(file_path)\n",
        "df_cv = pd.DataFrame({\"Resume_str\": [cv_text]})\n",
        "\n",
        "# Predict\n",
        "df_results = predict_cv_dataframe(df_cv, JOB_DESCRIPTION_SAMPLE, jd_result[\"skills\"])\n",
        "\n",
        "valid_cvs = df_results[df_results['pred_label'] == 1]\n",
        "if valid_cvs.empty: print(\"CV is not suitable | CV kh√¥ng ph√π h·ª£p.\")\n",
        "\n",
        "for _, row in valid_cvs.iterrows():\n",
        "    print(\"\\nSkills in JD:\", jd_result[\"skills\"])\n",
        "    print(\"Skills in CV:\", row['extracted_skills'])\n",
        "\n",
        "    # Encode CV + Questions\n",
        "    sw_q_df = pd.read_csv(\"./data/software_questions.csv\", encoding=\"windows-1252\")\n",
        "    sw_embeddings = MULTI_EMBED_MODEL.encode(sw_q_df['Question'].astype(str).tolist(), convert_to_tensor=True)\n",
        "    cv_embedding = EMBED_MODEL.encode([row['cv_text']], convert_to_tensor=True)\n",
        "\n",
        "    print(\"=== Suggested Software Questions ===\")\n",
        "    print_matched_questions(cv_embedding, sw_embeddings, sw_q_df, threshold=0.15, top_k=5)\n"
      ],
      "metadata": {
        "id": "N_86tR-qMiVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}